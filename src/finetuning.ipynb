{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import PreTrainedTokenizerFast, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from transformers import RobertaTokenizer\n",
    "import evaluate\n",
    "from torch.optim import AdamW\n",
    "from transformers import RobertaForSequenceClassification, Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "TEST_DATASET_LENGTH = 100\n",
    "VAL_DATASET_LENGTH = TEST_DATASET_LENGTH + 100\n",
    "\n",
    "USE_SMALL_DATASET = True\n",
    "BATCH_SIZE = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset from datasets/strategyqa_train_filtered.json\n",
    "# dataset = load_dataset(\"json\", data_files={\"train\": \"../datasets/strategyqa_train_filtered.json\", \"test\": \"../datasets/strategyqa_test.json\"})\n",
    "dataset = load_dataset(\"json\", data_files={\"train\": \"../datasets/strategyqa_train_filtered.json\"})\n",
    "print(dataset)\n",
    "# initialize training, validation, and testing dataset\n",
    "train_dataset = dataset['train'].select(indices=range(len(dataset['train']) - VAL_DATASET_LENGTH))\n",
    "val_dataset = dataset['train'].select(indices=range(len(dataset['train']) - VAL_DATASET_LENGTH, len(dataset['train']) - TEST_DATASET_LENGTH))\n",
    "test_dataset = dataset['train'].select(indices=range(len(dataset['train']) - TEST_DATASET_LENGTH, len(dataset['train'])))\n",
    "if USE_SMALL_DATASET:\n",
    "    train_dataset = train_dataset.select(range(100)) # we use the first 100 entries to test the code\n",
    "    val_dataset = val_dataset.select(range(100)) # we use the first 100 entries to test the code\n",
    "    test_dataset = test_dataset.select(range(100)) # we use the first 100 entries to test the code\n",
    "print(len(train_dataset))\n",
    "print(len(val_dataset))\n",
    "print(len(test_dataset))\n",
    "print(train_dataset[0])\n",
    "print(val_dataset[0])\n",
    "print(test_dataset[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# load tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "# tokenize the dataset\n",
    "def tokenize_function(batch, tokenizer=tokenizer, field_name=\"question\"):\n",
    "    return tokenizer(batch[field_name], padding=\"max_length\", truncation=True)\n",
    "\n",
    "def add_labels(tokenized_dataset):\n",
    "    tokenized_dataset[\"labels\"] = 1 if tokenized_dataset[\"answer\"] else 0 # Assuming \"answer\" exists\n",
    "    print(tokenized_dataset[\"labels\"], tokenized_dataset[\"answer\"], tokenized_dataset[\"question\"])\n",
    "    return tokenized_dataset\n",
    "# load training dataset\n",
    "\n",
    "\n",
    "# tokenize the datasets\n",
    "tokenized_datasets = {}\n",
    "tokenized_datasets[\"train\"] = train_dataset.map(tokenize_function, batched=True).map(add_labels)\n",
    "tokenized_datasets[\"val\"] = val_dataset.map(tokenize_function, batched=True).map(add_labels)\n",
    "tokenized_datasets[\"test\"] = test_dataset.map(tokenize_function, batched=True).map(add_labels)\n",
    "print(tokenized_datasets[\"train\"][0][\"labels\"])\n",
    "print(tokenized_datasets[\"train\"].column_names)\n",
    "# print(tokenized_datasets[\"train\"][98][\"labels\"], tokenized_datasets[\"train\"][98][\"question\"])\n",
    "for i in range(TEST_DATASET_LENGTH):\n",
    "    print(i, tokenized_datasets[\"val\"][i][\"labels\"], tokenized_datasets[\"val\"][i][\"question\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# %pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# check if GPU is available\n",
    "# ! nvidia-smi\n",
    "# ! nvcc --version\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "print(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# ! pip install torch torchvision torchaudio accelerate>=0.26.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, optimizer, epoch):\n",
    "    model.train()\n",
    "\n",
    "    with tqdm(dataloader, desc=f\"Train Ep {epoch}\", total=len(dataloader)) as tq:\n",
    "        for batch in tq:\n",
    "            # TODO: retrieve the data from your batch and send it to the same device as your model (i.e., model.device).\n",
    "            # Hint: model.device should point to 'cuda' as you set it as such in the main function below.\n",
    "            #       However, please use `model.device` and don't hard code it to 'cuda' as the auto-grader will put the model on CPU.\n",
    "            # text_encoding = {key: val.to(model.device) for key, val in batch.items() if key != \"labels\"}\n",
    "            input_ids = batch[\"text_encoding\"][\"input_ids\"].to(model.device)\n",
    "            attention_mask = batch[\"text_encoding\"][\"attention_mask\"].to(model.device)\n",
    "            label_encoding = batch[\"label_encoding\"].to(model.device)\n",
    "\n",
    "            # TODO: Compute loss by running model with text_encoding and label_encoding.\n",
    "            output = model(input_ids=input_ids, attention_mask=attention_mask, labels=label_encoding)\n",
    "            loss = output.loss\n",
    "\n",
    "            # TODO: compute gradients and update parameters using optimizer.\n",
    "            # Hint: you need three lines of code here!\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            tq.set_postfix({\"loss\": loss.detach().item()}) # for printing better-looking progress bar\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \n",
    "    logits, labels = eval_pred\n",
    "    predictions = logits.argmax(axis=-1)  # Convert logits to class labels\n",
    "    print(\"labels\")\n",
    "    print(labels)\n",
    "    # print(\"logits\")\n",
    "    # for logit in logits:\n",
    "    #     print([f\"{value:.2f}\" for value in logit])\n",
    "    print(\"predictions\")\n",
    "    print(predictions)\n",
    "    print()\n",
    "    for i in range(len(predictions)):\n",
    "        print(f\"Prediction: {predictions[i]} | Label: {labels[i]} | Sentence: {tokenized_datasets[\"test\"][i][\"question\"]}\")\n",
    "    return {\"accuracy\": (predictions == labels).mean()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# training\n",
    "\n",
    "learning_rate = 5e-5\n",
    "num_train_epochs = 5\n",
    "\n",
    "\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=2)\n",
    "\n",
    "model = model.cuda()\n",
    "\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "training_args = TrainingArguments(\"results\", \n",
    "                                num_train_epochs=num_train_epochs, \n",
    "                                per_device_train_batch_size=BATCH_SIZE, \n",
    "                                per_device_eval_batch_size=BATCH_SIZE, \n",
    "                                logging_dir= 'logs', \n",
    "                                logging_steps=10, \n",
    "                                evaluation_strategy= \"epoch\")\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "trainer = Trainer(model=model, \n",
    "                args=training_args, \n",
    "                train_dataset=tokenized_datasets[\"train\"], \n",
    "                eval_dataset=tokenized_datasets[\"val\"], \n",
    "                compute_metrics=compute_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# This cell clears GPU memory, do this when GPU out of memory\n",
    "\n",
    "# from numba import cuda\n",
    "import gc\n",
    "gc.collect()\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# if loading from a checkpoint, set load_model to True\n",
    "\n",
    "load_model = False\n",
    "if load_model:\n",
    "    checkpoint_path = \"./results/checkpoint-first\"\n",
    "\n",
    "    # Load model from a specific checkpoint\n",
    "    model = RobertaForSequenceClassification.from_pretrained(checkpoint_path)\n",
    "\n",
    "    trainer = Trainer(model=model, \n",
    "                    args=training_args, \n",
    "                    train_dataset=tokenized_datasets[\"train\"], \n",
    "                    eval_dataset=tokenized_datasets[\"val\"], \n",
    "                    compute_metrics=compute_metrics)\n",
    "\n",
    "trainer.evaluate(tokenized_datasets[\"test\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "def predict_factually_correct(input_text):\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "\n",
    "    # Move inputs to GPU if available\n",
    "    inputs = {key: value.cuda() for key, value in inputs.items()} if torch.cuda.is_available() else inputs\n",
    "\n",
    "    # Get model predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Get the logits and apply softmax to get probabilities\n",
    "    logits = outputs.logits\n",
    "    probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
    "\n",
    "    # Get the predicted class (index of the highest probability)\n",
    "    predicted_class = torch.argmax(probabilities, dim=-1).item()\n",
    "\n",
    "    # Map predicted class to \"correct\" or \"incorrect\"\n",
    "    if predicted_class == 1:\n",
    "        return \"Factually Correct\"\n",
    "    else:\n",
    "        return \"Factually Incorrect\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# we can now use the model to predict the factuality of a given sentence, go play with it!\n",
    "user_input = \"Was the KGB responsible for Lincoln's assassination?\"\n",
    "prediction = predict_factually_correct(user_input)\n",
    "print(f\"The sentence is: {prediction}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
